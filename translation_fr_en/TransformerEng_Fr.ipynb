{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNXMWl5GvjxQ26nqJEwuLJL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I build a transformer using PyTorch to translate sentences from French to English, given a large text file of various translations."
      ],
      "metadata": {
        "id": "ZWa4Xv1wxI0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA9wW-B_2QY6",
        "outputId": "3db8cdcb-fdc7-46e9-f9d2-419aa0a0a7c2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wBDBXMEpw04w"
      },
      "outputs": [],
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the transformer contains an encoder and a decoder. Unlike vanilla encoder/decoders with Recurrent Neural Networks (RNNs), the transformer both working in parallel.\n",
        "\n",
        "However, to start off, we need the building blocks, the principal of which is multi-head attention.\n",
        "\n",
        "This consists of multiple attention heads, as the name goes. A single attention head uses a set containing a query, key and value, which were learned during training.\n",
        "\n",
        "A query is dataset dependent, and is part of the model's search for a pattern that commonly recurrs within sentences. Once the pattern is discovered, it applies attention weights to those parts of the sentence."
      ],
      "metadata": {
        "id": "qvQtFN1C2a9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # matrix of query vectors (multiple heads)\n",
        "        self.W_k = nn.Linear(d_model, d_model) # key matrix\n",
        "        self.W_v = nn.Linear(d_model, d_model) # values matrix\n",
        "        self.W_o = nn.Linear(d_model, d_model) # output weights matrix\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output"
      ],
      "metadata": {
        "id": "XwIYIxSQ3ic-"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the Position-wise FFN (Feed-Forward Network). It will refine the representations of the sentence.\n"
      ],
      "metadata": {
        "id": "2Amqozx4bbIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFFN(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFFN, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff) # fully connected (FC) linear layer\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "KhQ7F5Jk3DrL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, since attention is applied in parallel, we need to store information regarding the relative positions of words.\n",
        "\n",
        "Following the original transformer paper, positions of words will be encoded as sine and cosine functions of frequencies that correspond to their positions and the dimensions of the word embedding space.\n",
        "\n"
      ],
      "metadata": {
        "id": "vmY7JMsf3uf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "ke2-vdi34SWM"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the transformer's encoder consists of: multi-head attention, feed-forward, and layer normalization. The layer norm will statistically normalize the output of the encoder's FFN so stabilize and accelerate training."
      ],
      "metadata": {
        "id": "1bQ4q7hB6-wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFFN(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model) # normalizes attentions to avoid skewed data\n",
        "        self.norm2 = nn.LayerNorm(d_model) # mitigates exploding/vanishing gradients\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "lz5kL_BP7rtY"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFFN(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "VwqQmRo79C8y"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining encoder & decoder with a final linear layer, we obtain the output probabilities for various words."
      ],
      "metadata": {
        "id": "ALuWv6U8CHzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Transformer(nn.Module): # meant for translating language to language\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [Encoder(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.decoder_layers = nn.ModuleList(\n",
        "            [Decoder(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2) # expand dimension of source mask\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3) # expand dimension of mask\n",
        "        seq_length = tgt.size(1)\n",
        "\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=device), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "A5SIaCt4CQZP"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have functions to prepare the text file and tokenize the translations."
      ],
      "metadata": {
        "id": "UW0hU9L9QxuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Lang:\n",
        "    def __init__(self, file_path, max_len):\n",
        "        self.src_sentences, self.tgt_sentences = self.load_data(file_path)\n",
        "        self.src_vocab, self.src_tokenizer = self.build_vocab(self.src_sentences)\n",
        "        self.tgt_vocab, self.tgt_tokenizer = self.build_vocab(self.tgt_sentences)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def load_data(self, file_path):\n",
        "        src_sentences = []\n",
        "        tgt_sentences = []\n",
        "        with open(file_path, encoding = 'utf-8') as f:\n",
        "            for line in f:\n",
        "                pair = line.strip().split('\\t')\n",
        "                if len(pair) == 2:\n",
        "                    tgt_sentences.append(pair[0])  # English (target)\n",
        "                    src_sentences.append(pair[1])  # French (source)\n",
        "        return src_sentences, tgt_sentences\n",
        "\n",
        "    def tokenize(self, sentence):\n",
        "        return re.findall(r\"\\b\\w+\\b\", sentence.lower())\n",
        "\n",
        "    def build_vocab(self, sentences):\n",
        "        tokenized_sentences = [self.tokenize(s) for s in sentences]\n",
        "        vocab_counter = Counter(token for sent in tokenized_sentences for token in sent)\n",
        "        \"\"\"\n",
        "        Meaning of special tokens:\n",
        "        - <pad> - equalize sentence length\n",
        "        - <sos> - begin each sentence\n",
        "        - <eos> - end each sentence\n",
        "        - <unk> - unrecognized\n",
        "        \"\"\"\n",
        "        vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
        "        vocab.update({word: i + 4 for i, (word, _) in enumerate(vocab_counter.most_common())})\n",
        "        return vocab, tokenized_sentences\n",
        "\n",
        "    def encode_sentence(self, sentence, vocab):\n",
        "        tokens = self.tokenize(sentence)\n",
        "        token_ids = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
        "        token_ids = [vocab['<sos>']] + token_ids[:self.max_len-2] + [vocab['<eos>']]\n",
        "        return token_ids + [vocab['<pad>']] * (self.max_len - len(token_ids))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_encoded = self.encode_sentence(self.src_sentences[idx], self.src_vocab)\n",
        "        tgt_encoded = self.encode_sentence(self.tgt_sentences[idx], self.tgt_vocab)\n",
        "        return torch.tensor(src_encoded), torch.tensor(tgt_encoded)"
      ],
      "metadata": {
        "id": "eVYjWQYXQ1_v"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The file is uploaded, and then moved by the intermediate line below."
      ],
      "metadata": {
        "id": "AIutNkN1kUD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mv ../eng-fra.txt ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eMmiX7zkW3K",
        "outputId": "8cd33d18-edaa-48d3-f0f6-09f297520b51"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '../eng-fra.txt': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"eng-fra.txt\"\n",
        "max_len = 50\n",
        "batch_size = 64  # Adjust as needed\n",
        "\n",
        "dataset = Lang(file_path, max_len)\n",
        "src_vocab_size = len(dataset.src_vocab)\n",
        "tgt_vocab_size = len(dataset.tgt_vocab)\n",
        "\n",
        "# Model hyperparameters\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "dropout = 0.1\n",
        "num_epochs = 10\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Initialize model\n",
        "model = Transformer(\n",
        "    src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout\n",
        ").to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.src_vocab[\"<pad>\"]).cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Manually batching data\n",
        "def get_batches(dataset, batch_size):\n",
        "    indices = list(range(len(dataset)))\n",
        "    random.shuffle(indices)\n",
        "    for i in range(0, len(indices), batch_size):\n",
        "        batch_indices = indices[i : i + batch_size]\n",
        "        src_batch = torch.stack([dataset[idx][0] for idx in batch_indices]).to(device)\n",
        "        tgt_batch = torch.stack([dataset[idx][1] for idx in batch_indices]).to(device)\n",
        "        yield src_batch, tgt_batch\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for src_batch, tgt_batch in get_batches(dataset, batch_size):\n",
        "        tgt_input = tgt_batch[:, :-1].to(device)  # Remove <eos> for decoder input\n",
        "        tgt_output = tgt_batch[:, 1:].reshape(-1).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = model(src_batch, tgt_input).reshape(-1, tgt_vocab_size)  # Shape [batch * seq_len, vocab_size]\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(predictions, tgt_output)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "3wnNxqE9jppL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "489798e4-304f-4202-bf26-b911ef72b0a1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 12225.1188\n",
            "Epoch [2/10], Loss: 12142.0027\n",
            "Epoch [3/10], Loss: 12133.2155\n",
            "Epoch [4/10], Loss: 12126.6601\n",
            "Epoch [5/10], Loss: 12123.5837\n",
            "Epoch [6/10], Loss: 12118.5239\n",
            "Epoch [7/10], Loss: 12112.9262\n",
            "Epoch [8/10], Loss: 12101.0533\n",
            "Epoch [9/10], Loss: 12093.2505\n",
            "Epoch [10/10], Loss: 12087.0391\n",
            "Training complete!\n"
          ]
        }
      ]
    }
  ]
}