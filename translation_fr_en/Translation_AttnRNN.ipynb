{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source: tutorial from PyTorch. https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
      ],
      "metadata": {
        "id": "Mmll87Jy6TaN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vTf9YABojrAR"
      },
      "outputs": [],
      "source": [
        "# For requirements\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need functions to process the lines of text:"
      ],
      "metadata": {
        "id": "P-nRx8Zz5KNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0 # start of sequence\n",
        "EOS_token = 1 # end of sequence\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {} # dictionaries about the corpus received\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "metadata": {
        "id": "TzRdFH565fD6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform unicode to ASCII\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter chars\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "    return s.strip()"
      ],
      "metadata": {
        "id": "Z2_StAhF6Zgc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# process the file (which has been moved into the current directory in colab)\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # read file and split lines\n",
        "    lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # split each line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n"
      ],
      "metadata": {
        "id": "AXGdF1ss8jQQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "# we will train only on sentences starting with any of these...others will take longer\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "metadata": {
        "id": "U3uHaqj9-uYa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mv ../eng-fra.txt ."
      ],
      "metadata": {
        "id": "hdCmGy3yxiEG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# full process defined from here:\n",
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "\n",
        "    print(\"Counted words: \")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "# called below:\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-Cf4Gfk_WJ1",
        "outputId": "ad2e62b0-9a4c-4797-bbc0-79e554caa666"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 25775 sentence pairs\n",
            "Trimmed to 4498 sentence pairs\n",
            "Counting words...\n",
            "Counted words: \n",
            "fra 2175\n",
            "eng 1253\n",
            "['c est un menteur', 'he s a liar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sequence-to-sequence encoder is below. For each word the cell's output is a vector and a hidden state. The hidden state is used to process the next word.\n",
        "\n",
        "It contains an embedding layer to train vector representations of words (via a lookup table), and a Gated Recurrent Unit (GRU) to learn sequences."
      ],
      "metadata": {
        "id": "CwYxF3bLCx6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "060ODJU6C5xg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # feed target to next input\n",
        "            else:\n",
        "                # use predictions\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs, decoder_hidden, None\n",
        "\n",
        "    def forward_step(self, input, hidden):\n",
        "        output = self.embedding(input)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "g6ylDw_YcvcB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The section below implementions Additive Attention, aka [Bahdanau Attention](https://paperswithcode.com/paper/neural-machine-translation-by-jointly). It uses a feed-forward network to calculate attention alignment scores and then applying softmax."
      ],
      "metadata": {
        "id": "tSsbSKnQl_su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size) # Weighted attention\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size) # Unweighted Attention\n",
        "        self.Va = nn.Linear(hidden_size, 1) # Value attention\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token) # initialize empty input\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # teacher forcing\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
        "            else:\n",
        "                # no teacher forcing\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2) # reorder dims - query is encoder's hidden layer\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ],
      "metadata": {
        "id": "EqUOxWzWmO-A"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: implement attention from [this](https://arxiv.org/abs/1508.04025) paper."
      ],
      "metadata": {
        "id": "yvQkHiWNxEYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, helper functions to prepare training data."
      ],
      "metadata": {
        "id": "gwZFYaUGxJlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ' )]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "def get_dataloader(batch_size):\n",
        "    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    return input_lang, output_lang, train_dataloader\n",
        "\n",
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss/ len(dataloader)"
      ],
      "metadata": {
        "id": "y61gk-fuxMk2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "metadata": {
        "id": "l00f0T6_4l1T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=100, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0\n",
        "    plot_loss_total = 0\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss() # negative log likelihood loss\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "metadata": {
        "id": "tDXc8eyb4k8A"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "metadata": {
        "id": "Vftf2Ms65fw5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU6FZfdP5mSA",
        "outputId": "5846d112-d3aa-4c18-e217-ba6d45e87864"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 25775 sentence pairs\n",
            "Trimmed to 4498 sentence pairs\n",
            "Counting words...\n",
            "Counted words: \n",
            "fra 2175\n",
            "eng 1253\n",
            "0m 14s (- 3m 44s) (5 6%) 1.1851\n",
            "0m 29s (- 3m 29s) (10 12%) 0.5943\n",
            "0m 44s (- 3m 12s) (15 18%) 0.3515\n",
            "0m 59s (- 2m 57s) (20 25%) 0.1857\n",
            "1m 13s (- 2m 42s) (25 31%) 0.0964\n",
            "1m 29s (- 2m 28s) (30 37%) 0.0569\n",
            "1m 43s (- 2m 13s) (35 43%) 0.0408\n",
            "1m 58s (- 1m 58s) (40 50%) 0.0336\n",
            "2m 12s (- 1m 43s) (45 56%) 0.0300\n",
            "2m 27s (- 1m 28s) (50 62%) 0.0284\n",
            "2m 42s (- 1m 13s) (55 68%) 0.0257\n",
            "2m 57s (- 0m 59s) (60 75%) 0.0247\n",
            "3m 11s (- 0m 44s) (65 81%) 0.0241\n",
            "3m 26s (- 0m 29s) (70 87%) 0.0232\n",
            "3m 41s (- 0m 14s) (75 93%) 0.0223\n",
            "3m 56s (- 0m 0s) (80 100%) 0.0221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words, decoder_attn"
      ],
      "metadata": {
        "id": "owSvM1FxSbjp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "metadata": {
        "id": "oOHOvFWdWT6p"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.eval() # switch modes\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTpgL8DQWm7a",
        "outputId": "c4ac9127-7b2e-4d6a-8256-5210c159f570"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> tu es tres avise\n",
            "= you re very wise\n",
            "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> tu es celui la\n",
            "= you are the one\n",
            "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> elles sont en colere apres toi\n",
            "= they re mad at you\n",
            "< she s mad at you <EOS> EOS SOS SOS SOS SOS\n",
            "\n",
            "> ce sont des animaux\n",
            "= they re animals\n",
            "< he s animals easygoing awake awake awake <EOS> EOS SOS SOS\n",
            "\n",
            "> je suis accro\n",
            "= i m addicted\n",
            "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> c est un enfant mince\n",
            "= he is a slim child\n",
            "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> je suis faineante\n",
            "= i am lazy\n",
            "< i m lazy <EOS> EOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> je suis chanceuse\n",
            "= i m lucky\n",
            "< i m lucky <EOS> EOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> nous sommes bourrees\n",
            "= we re smashed\n",
            "< we re sloshed <EOS> EOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> elles sont furieuses apres vous\n",
            "= they re mad at you\n",
            "< i m mad at you <EOS> EOS SOS SOS SOS SOS\n",
            "\n"
          ]
        }
      ]
    }
  ]
}